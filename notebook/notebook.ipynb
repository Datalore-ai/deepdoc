{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from typing import TypedDict, List, Annotated, Literal, Union\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import operator\n",
    "\n",
    "from langgraph.types import Command, Send\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import uuid\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(\n",
    "        provider: Literal[\"openai\", \"anthropic\", \"google\", \"ollama\"],\n",
    "        model: str,\n",
    "        temperature: float = 0.5,\n",
    "):\n",
    "    if provider == \"openai\":\n",
    "        return ChatOpenAI(model=model, temperature=temperature)\n",
    "    elif provider == \"anthropic\":\n",
    "        return ChatAnthropic(model=model, temperature=temperature)\n",
    "    elif provider == \"google\":\n",
    "        return ChatGoogleGenerativeAI(model=model, temperature=temperature)\n",
    "    elif provider == \"ollama\":\n",
    "        return ChatOllama(model=model, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_llm(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    section_name: str = Field(..., description=\"The name of this section of the report without its number\")\n",
    "    sub_sections: List[str] = Field(..., description=\"Comprehensive descriptions of sub-sections, each combining the sub-section title and its bullet points into a fluid, natural-language description\")\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(..., description=\"A list of sections\")\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query: str = Field(..., description=\"A search query\")\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[Query] = Field(..., description=\"A list of search queries\")\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    query: Query = Field(..., description=\"The search query that was used to retrieve the raw content\")\n",
    "    raw_content: list[str] = Field(..., description=\"The raw content retrieved from the search\")\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    feedback: Union[str, bool] = Field(..., description=\"Feedback on the report structure. If the content is good for the section, return True (boolean), otherwise return a string of feedback on what is missing or incorrect.\")\n",
    "\n",
    "class SectionOutput(BaseModel):\n",
    "    final_section_content: List[str] = Field(..., description=\"The final section content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    topic: str\n",
    "    outline: str\n",
    "    resource_path: str\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    report_structure: str\n",
    "    sections: List[Section]\n",
    "    final_section_content: Annotated[List[str], operator.add] = []\n",
    "    final_report_content: str\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    section: Section\n",
    "    knowledge: str\n",
    "    reflection_feedback: Feedback = Feedback(feedback=\"\")\n",
    "    generated_queries: List[Query] = []\n",
    "    searched_queries: Annotated[List[Query], operator.add] = []\n",
    "    search_results: Annotated[List[SearchResult], operator.add] = []\n",
    "    accumulated_content: str = \"\"\n",
    "    reflection_count: int = 1\n",
    "    final_section_content: Annotated[List[str], operator.add] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from qdrant_setup import rag_pipeline_setup\n",
    "from chunk_prep import create_chunks\n",
    "\n",
    "def resource_setup_node(state: AgentState, config: RunnableConfig):\n",
    "    thread_id = config.get(\"configurable\").get(\"thread_id\")\n",
    "    directory_path = state.get(\"resource_path\")\n",
    "    chunks = create_chunks(directory_path)\n",
    "    rag_pipeline_setup(thread_id, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_structure_planner_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(REPORT_STRUCTURE_PLANNER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "        Topic: {topic}\n",
    "        Outline: {outline}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "report_structure_planner_llm = report_structure_planner_system_prompt | llm\n",
    "\n",
    "def report_structure_planner_node(state: AgentState, config: RunnableConfig):\n",
    "    result = report_structure_planner_llm.invoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_node(state: AgentState, config: RunnableConfig)->Command[Literal[\"section_formatter\", \"report_structure_planner\"]]:\n",
    "    human_message = input(\"Please provide feedback on the report structure (type 'continue' to continue): \")\n",
    "    report_structure = state.get(\"messages\")[-1].content\n",
    "    if human_message == \"continue\":\n",
    "        return Command(\n",
    "            goto=\"section_formatter\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)], \"report_structure\": report_structure}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"report_structure_planner\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)]}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "section_formatter_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{report_structure}\"),\n",
    "])\n",
    "\n",
    "section_formatter_llm = section_formatter_system_prompt | llm.with_structured_output(Sections)\n",
    "\n",
    "def section_formatter_node(state: AgentState, config: RunnableConfig) -> Command[Literal[\"research_agent\"]]:\n",
    "    result = section_formatter_llm.invoke(state)\n",
    "    # return {\"sections\": result.sections}\n",
    "    return Command(\n",
    "        update={\"sections\": result.sections},\n",
    "        goto=[\n",
    "            Send(\n",
    "                \"research_agent\",\n",
    "                {\n",
    "                    \"section\": s,\n",
    "                }\n",
    "            ) for s in result.sections\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_knowledge_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(SECTION_KNOWLEDGE_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{section}\"),\n",
    "])\n",
    "\n",
    "section_knowledge_llm = section_knowledge_system_prompt | llm\n",
    "\n",
    "def section_knowledge_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = section_knowledge_llm.invoke(state)\n",
    "    return {\"knowledge\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generator_node(state: ResearchState, config: RunnableConfig):\n",
    "    query_generator_system_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(QUERY_GENERATOR_SYSTEM_PROMPT_TEMPLATE),\n",
    "        HumanMessagePromptTemplate.from_template(template=\"Section: {section}\\nPrevious Queries: {searched_queries}\\nReflection Feedback: {reflection_feedback}\"),\n",
    "    ])\n",
    "\n",
    "    query_generator_llm = query_generator_system_prompt | llm.with_structured_output(Queries)\n",
    "    state.setdefault(\"reflection_feedback\", \"\")\n",
    "    state.setdefault(\"searched_queries\", [])\n",
    "    configurable = config.get(\"configurable\")\n",
    "\n",
    "    input_data = {\n",
    "        **state,\n",
    "        **configurable  # includes max_queries, search_depth, etc.\n",
    "    }\n",
    "\n",
    "    result = query_generator_llm.invoke(input_data, configurable)\n",
    "    return {\"generated_queries\": result.queries, \"searched_queries\": result.queries}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_setup import retrieve_from_store\n",
    "\n",
    "def rag_search_node(state: ResearchState, config: RunnableConfig):\n",
    "    queries = state[\"generated_queries\"]\n",
    "    configurable = config.get(\"configurable\")\n",
    "    search_results = []\n",
    "    for query in queries:\n",
    "        raw_content = []\n",
    "        response = retrieve_from_store(query.query, configurable.get(\"thread_id\"), configurable.get(\"n_points\"))\n",
    "        for result in response:\n",
    "            content = f\"filename:{result.payload['document']['filename']}\\nPage_number:{result.payload['document']['page_number']}\\nPage_Content: {result.payload['document'][\"page_content\"]}\\n\\n\\n\"\n",
    "            raw_content.append(content)\n",
    "        search_results.append(SearchResult(query=query, raw_content=raw_content))\n",
    "    return {\"search_results\": search_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_accumulator_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(RESULT_ACCUMULATOR_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{search_results}\"),\n",
    "])\n",
    "\n",
    "result_accumulator_llm = result_accumulator_system_prompt | llm\n",
    "\n",
    "def result_accumulator_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = result_accumulator_llm.invoke(state)\n",
    "    return {\"accumulated_content\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_feedback_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(REFLECTION_FEEDBACK_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Section: {section}\\nAccumulated Content: {accumulated_content}\"),\n",
    "])\n",
    "\n",
    "reflection_feedback_llm = reflection_feedback_system_prompt | llm.with_structured_output(Feedback)\n",
    "\n",
    "def reflection_feedback_node(state: ResearchState, config: RunnableConfig) -> Command[Literal[\"final_section_formatter\", \"query_generator\"]]:\n",
    "    reflection_count = state.get(\"reflection_count\", 0)\n",
    "    configurable = config.get(\"configurable\")\n",
    "    result = reflection_feedback_llm.invoke(state)\n",
    "    feedback = result.feedback\n",
    "    if (feedback == True) or (feedback.lower() == \"true\") or (reflection_count < configurable.get(\"num_reflections\")):\n",
    "        return Command(\n",
    "            update={\"reflection_feedback\": feedback},\n",
    "            goto=\"final_section_formatter\"\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            update={\"reflection_feedback\": feedback, \"reflection_count\": reflection_count + 1},\n",
    "            goto=\"query_generator\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_section_formatter_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(FINAL_SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Internal Knowledge: {knowledge}\\nSearch Result content: {accumulated_content}\"),\n",
    "])\n",
    "\n",
    "final_section_formatter_llm = final_section_formatter_system_prompt | llm\n",
    "\n",
    "def final_section_formatter_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = final_section_formatter_llm.invoke(state)\n",
    "    return {\"final_section_content\": [result.content]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report_writer_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(FINAL_REPORT_WRITER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Report Structure: {report_structure}\\nSection Contents: {final_section_content}\"),\n",
    "])\n",
    "\n",
    "final_report_writer_llm = final_report_writer_system_prompt | llm\n",
    "\n",
    "def final_report_writer_node(state: AgentState, config: RunnableConfig):\n",
    "    result = final_report_writer_llm.invoke(state)\n",
    "    return {\"final_report_content\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_builder = StateGraph(ResearchState, output=SectionOutput)\n",
    "\n",
    "research_builder.add_node(\"section_knowledge\", section_knowledge_node)\n",
    "research_builder.add_node(\"query_generator\", query_generator_node)\n",
    "research_builder.add_node(\"rag_search\", rag_search_node)\n",
    "research_builder.add_node(\"result_accumulator\", result_accumulator_node)\n",
    "research_builder.add_node(\"reflection\", reflection_feedback_node)\n",
    "research_builder.add_node(\"final_section_formatter\", final_section_formatter_node)\n",
    "\n",
    "research_builder.add_edge(START, \"section_knowledge\")\n",
    "research_builder.add_edge(\"section_knowledge\", \"query_generator\")\n",
    "research_builder.add_edge(\"query_generator\", \"rag_search\")\n",
    "research_builder.add_edge(\"rag_search\", \"result_accumulator\")\n",
    "research_builder.add_edge(\"result_accumulator\", \"reflection\")\n",
    "research_builder.add_edge(\"final_section_formatter\", END)\n",
    "\n",
    "memory_saver = MemorySaver()\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"resource_setup\", resource_setup_node)\n",
    "builder.add_node(\"report_structure_planner\", report_structure_planner_node)\n",
    "builder.add_node(\"human_feedback\", human_feedback_node)\n",
    "builder.add_node(\"section_formatter\", section_formatter_node)\n",
    "builder.add_node(\"research_agent\", research_builder.compile())\n",
    "builder.add_node(\"final_report_writer\", final_report_writer_node)\n",
    "\n",
    "builder.set_entry_point(\"resource_setup\")\n",
    "builder.add_edge(\"resource_setup\", \"report_structure_planner\")\n",
    "builder.add_edge(\"report_structure_planner\", \"human_feedback\")\n",
    "builder.add_edge(\"research_agent\", \"final_report_writer\")\n",
    "builder.add_edge(\"final_report_writer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory_saver)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"Research paper details\"\n",
    "OUTLINE = \"I want some details on this research paper.\"\n",
    "\n",
    "thread = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": str(uuid.uuid4()),\n",
    "        \"max_queries\": 3,\n",
    "        \"search_depth\": 2,\n",
    "        \"num_reflections\": 2,\n",
    "        \"n_points\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "for event in graph.stream(\n",
    "    {\"topic\": TOPIC, \"outline\": OUTLINE, \"resource_path\": \"./resource\"},\n",
    "    config=thread,\n",
    "):\n",
    "    if \"resource_setup\" in event:\n",
    "        print(\"<<< RESOURCE SETUP >>>\")\n",
    "        print(\"Setting up the database for you...\\n\")\n",
    "    elif \"report_structure_planner\" in event:\n",
    "        print(\"<<< REPORT STRUCTURE PLANNER >>>\")\n",
    "        print(event[\"report_structure_planner\"][\"messages\"][-1].content)\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"section_formatter\" in event:\n",
    "        print(\"<<< SECTION FORMATTING >>>\")\n",
    "        print(event[\"section_formatter\"])\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"research_agent\" in event:\n",
    "        # check output of research_agent\n",
    "        print(\"<<< RESEARCH AGENT >>>\")\n",
    "        print(event[\"research_agent\"])\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"final_report_writer\" in event:\n",
    "        # check output of final_report_writer\n",
    "        print(\"<<< FINAL REPORT WRITER >>>\")\n",
    "        print(event[\"final_report_writer\"])\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    else:\n",
    "        print(\"<<< HUMAN FEEDBACK >>>\")\n",
    "        print(event[\"human_feedback\"][\"messages\"][-1].content)\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdoc (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
